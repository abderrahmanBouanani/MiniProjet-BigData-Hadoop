# Mini-Projet Big Data : Pipeline Hadoop

Ce d√©p√¥t contient l'infrastructure et le code source pour la "Conception et mise en ≈ìuvre d'une mini-cha√Æne Big Data bas√©e sur Hadoop".

## üìå √âtat d'avancement

* **√âtape 1 : Infrastructure & HDFS** (R√©alis√© par *Membre 1*)
* Environnement Dockeris√© (Hadoop NameNode + DataNode).
* Service HDFS et YARN op√©rationnels.


* Arborescence stricte cr√©√©e dans HDFS : `/user/hadoopuser/project/`.





## D√©marrage Rapide (Pour toute l'√©quipe)

1. **Cloner le projet :**
```bash
git clone https://github.com/VOTRE_USER/MiniProjet-BigData-Hadoop.git
cd MiniProjet-BigData-Hadoop

```


2. **Lancer l'environnement :**
```bash
docker-compose up -d

```


3. **V√©rifier l'acc√®s :**
* Interface Web HDFS : [http://localhost:9870](https://www.google.com/search?q=http://localhost:9870)
* V√©rifier les dossiers HDFS :
```bash
docker exec -it hadoop-master hdfs dfs -ls -R /user/hadoopuser/project/

```





---

## R√©partition des T√¢ches (To-Do List)

### Membre 2 : Ingestion de Logs (Apache Flume)

**Objectif :** Simuler et ing√©rer des logs en temps r√©el.

* [ ] Cr√©er un script Python (`scripts/generate_logs.py`) pour g√©n√©rer des logs al√©atoires.


* [ ] Configurer l'agent Flume (`config/flume.conf`) :
* 
**Source :** `exec` (commande qui lance le script python).


* 
**Channel :** `memory`.


* 
**Sink :** `HDFS`.




* [ ] **Livrable :** Les logs doivent appara√Ætre dans `/user/hadoopuser/project/logs`.



### Membre 3 : Ingestion Base de Donn√©es (Apache Sqoop)

**Objectif :** Importer des donn√©es relationnelles structur√©es.

* [ ] Configurer un conteneur MySQL (ajouter au `docker-compose.yml` ou installer localement).
* [ ] Cr√©er une table avec des donn√©es de test.
* [ ] Ex√©cuter l'import Sqoop vers `/user/hadoopuser/project/db_data`  avec :


* Un s√©parateur personnalis√©.


* Une clause `WHERE` pour filtrer les donn√©es.


* Utiliser au moins 2 mappers (`-m 2`).





### Membre 4 : Traitement (MapReduce / Hadoop Streaming)

**Objectif :** Analyser les donn√©es ing√©r√©es.

* [ ] D√©velopper `scripts/mapper.py` et `scripts/reducer.py`.
* [ ] Logique sugg√©r√©e : WordCount sur les logs ou comptage par cat√©gorie.


* [ ] Lancer le job avec **Hadoop Streaming**.


* [ ] **Livrable :** Les r√©sultats doivent √™tre stock√©s dans `/user/hadoopuser/project/output`.



### Membre 5 : Analyse & Rapport

**Objectif :** Interpr√©tation et consolidation.

* [ ] R√©cup√©rer les fichiers de sortie depuis HDFS (`hdfs dfs -get ...`).
* [ ] Cr√©er un script de visualisation (graphes) ou analyser les donn√©es.
* [ ] **Livrable :** R√©daction du rapport final (Word/PDF) incluant l'architecture, les codes, et l'interpr√©tation des r√©sultats.



---

## Structure du Projet

```text
‚îú‚îÄ‚îÄ config/             # Fichiers de configuration (flume.conf, etc.)
‚îú‚îÄ‚îÄ scripts/            # Scripts Python (logs, mapper, reducer)
‚îú‚îÄ‚îÄ sql/                # Scripts de cr√©ation de table MySQL
‚îú‚îÄ‚îÄ data_local/         # Donn√©es de test (ignor√© par git)
‚îú‚îÄ‚îÄ docker-compose.yml  # Configuration de l'infrastructure
‚îî‚îÄ‚îÄ README.md           # Documentation du projet

```

## Consignes de Collaboration (Git)

1. Ne travaillez jamais directement sur la branche `main`.
2. Cr√©ez une branche pour votre t√¢che : `git checkout -b feature/flume-ingestion`.
3. Faites vos commits et poussez votre branche.
4. Cr√©ez une **Pull Request** quand votre partie est pr√™te √† √™tre test√©e par le Membre 1.