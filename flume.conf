# ============================================================
# flume.conf - Configuration de l'agent Flume
# Membre 2 : Ingestion de Logs (Apache Flume)
# Mini-Projet Big Data - ENSA Agadir
# ============================================================
# Objectif : Capturer les logs générés par generate_logs.py
#            et les ingérer dans HDFS via Apache Flume
# ============================================================

# ------------------------------------------------------------
# 1. DÉCLARATION DES COMPOSANTS DE L'AGENT
# ------------------------------------------------------------
# Nom de l'agent : agent1
agent1.sources  = src_exec
agent1.channels = ch_memory
agent1.sinks    = sink_hdfs

# ------------------------------------------------------------
# 2. CONFIGURATION DE LA SOURCE (exec)
# ------------------------------------------------------------
# Type exec : exécute une commande shell et lit sa sortie stdout
agent1.sources.src_exec.type    = exec

# Commande qui lance le script Python de génération de logs
# Assurez-vous que le chemin vers generate_logs.py est correct
agent1.sources.src_exec.command = python3 /home/hadoopuser/project/scripts/generate_logs.py

# Redémarre la commande si elle s'arrête (tolérance aux pannes)
agent1.sources.src_exec.restart         = true
agent1.sources.src_exec.restartThrottle = 5000

# Encodage des données
agent1.sources.src_exec.charset = UTF-8

# Lier la source au channel
agent1.sources.src_exec.channels = ch_memory

# ------------------------------------------------------------
# 3. CONFIGURATION DU CHANNEL (memory)
# ------------------------------------------------------------
# Type memory : stockage temporaire en mémoire vive
agent1.channels.ch_memory.type                = memory

# Capacité maximale du channel (nombre d'événements en attente)
agent1.channels.ch_memory.capacity           = 10000

# Nombre maximum d'événements transmis par transaction
agent1.channels.ch_memory.transactionCapacity = 1000

# Mémoire allouée par événement (en octets)
agent1.channels.ch_memory.byteCapacityBufferPercentage = 20
agent1.channels.ch_memory.byteCapacity                = 800000

# ------------------------------------------------------------
# 4. CONFIGURATION DU SINK (HDFS)
# ------------------------------------------------------------
agent1.sinks.sink_hdfs.type    = hdfs
agent1.sinks.sink_hdfs.channel = ch_memory

# Chemin de destination dans HDFS
# %Y = année, %m = mois, %d = jour, %H = heure  (partitionnement temporel)
agent1.sinks.sink_hdfs.hdfs.path = hdfs://localhost:9000/user/hadoopuser/project/logs/%Y-%m-%d

# Préfixe et suffixe des fichiers créés dans HDFS
agent1.sinks.sink_hdfs.hdfs.filePrefix = app_logs_
agent1.sinks.sink_hdfs.hdfs.fileSuffix = .log

# Type de fichier : DataStream = texte brut (pas de compression Sequence File)
agent1.sinks.sink_hdfs.hdfs.fileType = DataStream

# Format de sérialisation : text = une ligne par événement
agent1.sinks.sink_hdfs.hdfs.writeFormat = Text

# ------------------------------------------------------------
# 5. POLITIQUE DE ROTATION DES FICHIERS
# ------------------------------------------------------------
# Rotation toutes les 60 secondes (0 = désactivé)
agent1.sinks.sink_hdfs.hdfs.rollInterval = 60

# Rotation tous les 10 Mo (0 = désactivé)
agent1.sinks.sink_hdfs.hdfs.rollSize = 10485760

# Rotation tous les 1000 événements (0 = désactivé)
agent1.sinks.sink_hdfs.hdfs.rollCount = 1000

# Nombre de threads d'écriture HDFS
agent1.sinks.sink_hdfs.hdfs.threadsPoolSize = 10

# Timeout de l'opération d'écriture (ms)
agent1.sinks.sink_hdfs.hdfs.callTimeout = 30000

# Taille du batch envoyé au sink par transaction
agent1.sinks.sink_hdfs.hdfs.batchSize = 100

# Utiliser les timestamps de l'événement pour le partitionnement
agent1.sinks.sink_hdfs.hdfs.useLocalTimeStamp = true

# ============================================================
# INSTRUCTIONS DE DÉMARRAGE
# ============================================================
#
# 1. Assurez-vous que Hadoop est démarré :
#    $ start-dfs.sh && start-yarn.sh
#
# 2. Créez le répertoire de destination dans HDFS :
#    $ hdfs dfs -mkdir -p /user/hadoopuser/project/logs
#
# 3. Démarrez l'agent Flume :
#    $ flume-ng agent \
#        --conf /etc/flume/conf \
#        --conf-file /home/hadoopuser/project/config/flume.conf \
#        --name agent1 \
#        -Dflume.root.logger=INFO,console
#
# 4. Vérifiez les fichiers ingérés dans HDFS :
#    $ hdfs dfs -ls /user/hadoopuser/project/logs/
#    $ hdfs dfs -cat /user/hadoopuser/project/logs/$(date +%Y-%m-%d)/app_logs_*.log
#
# ============================================================
